{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9eaf081b-20a2-433b-ac94-b0f2471be6fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "0. AZURE STORAGE CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a8f15af-c329-4089-858c-80417f807790",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "storage_account_name = \"gasprojectsadl\"\n",
    "processed_data_container = \"processed-data\"\n",
    "storage_key = dbutils.secrets.get(scope=\"gas-project-scope\", key=\"gasprojectsadl-key\")\n",
    "\n",
    "mount_point = f\"/mnt/{processed_data_container}\"\n",
    "if not any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts()):\n",
    "  dbutils.fs.mount(\n",
    "    source=f\"wasbs://{processed_data_container}@{storage_account_name}.blob.core.windows.net/\",\n",
    "    mount_point=mount_point,\n",
    "    extra_configs={f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\": storage_key},\n",
    "  )\n",
    "  print(f\"Mount point {mount_point} created.\")\n",
    "else:\n",
    "  print(f\"Mount point {mount_point} already exists.\")\n",
    "\n",
    "csv_folder_path = f\"/dbfs{mount_point}/final_ea_ldz_master_csv/\"\n",
    "try:\n",
    "    csv_file = [f for f in os.listdir(csv_folder_path) if f.endswith('.csv')][0]\n",
    "    master_data_path_dbfs = os.path.join(csv_folder_path, csv_file)\n",
    "    print(f\"Master data file path is ready: {master_data_path_dbfs}\")\n",
    "except IndexError:\n",
    "    print(f\"ERROR: No CSV file found in the directory: {csv_folder_path}\")\n",
    "\n",
    "PROCESSED_MOUNT = mount_point               \n",
    "PROCESSED_LOCAL = f\"/dbfs{mount_point}\"\n",
    "\n",
    "csv_folder_path = f\"{PROCESSED_LOCAL}/final_ea_ldz_master_csv/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82121c50-1a17-4af1-92c0-09e91942ed6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. GAS MODEL CLASS DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdb061e6-2f9f-4d5f-b1d7-a28d3ac60278",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Core Python Packages\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pmdarima.arima import auto_arima\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, SimpleRNN, GRU, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from contextlib import nullcontext\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    mean_absolute_percentage_error,\n",
    ")\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import tempfile\n",
    "import contextlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import mlflow\n",
    "    import mlflow.keras\n",
    "    try:\n",
    "        import mlflow.statsmodels\n",
    "        _HAS_SM_FLAVOR = True\n",
    "    except Exception:\n",
    "        _HAS_SM_FLAVOR = False\n",
    "    _MLFLOW_AVAILABLE = True\n",
    "except ImportError:\n",
    "    mlflow = None\n",
    "    _HAS_SM_FLAVOR = False\n",
    "    _MLFLOW_AVAILABLE = False\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class GasModel:\n",
    "    def __init__(self, data_path=None, dataframe=None):\n",
    "        \"\"\"\n",
    "        Initialize the GasModel with optional data source.\n",
    "        \"\"\"\n",
    "        if data_path:\n",
    "            self.df = pd.read_csv(data_path, parse_dates=True, index_col=0)\n",
    "        elif dataframe is not None:\n",
    "            self.df = dataframe.copy()\n",
    "        else:\n",
    "            self.df = None\n",
    "\n",
    "        self.model = None\n",
    "        self.history = {}\n",
    "        self.target_col = None\n",
    "        self.df_prepared = None\n",
    "        self.exog_cols = None\n",
    "        self.exog_future = pd.DataFrame()\n",
    "        self.future_forecast_series = None\n",
    "        self.processed_root = PROCESSED_LOCAL\n",
    "        self._active_model_name = None\n",
    "\n",
    "    ##################################################\n",
    "    # MLflow and artifact logging run helper\n",
    "    ##################################################\n",
    "\n",
    "    def _mlflow_run(self, enabled: bool, run_name: str): \n",
    "        \n",
    "        \"\"\"Return an active MLflow run context if enabled; otherwise a no-op context.\"\"\" \n",
    "        if enabled: \n",
    "            if not _MLFLOW_AVAILABLE: \n",
    "                raise ImportError(\"use_mlflow=True but MLflow isn't installed. Please pip install mlflow.\\n\") \n",
    "            return mlflow.start_run(run_name=run_name) \n",
    "        return nullcontext()\n",
    "    \n",
    "    def _safe_mkdir(self, p: Path) -> Path:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "        return p\n",
    "\n",
    "    def _active_run_stub(self) -> str:\n",
    "        \"\"\"Return a short run identifier (MLflow run_id[:8] if available, else timestamp).\"\"\"\n",
    "        rid = None\n",
    "        if _MLFLOW_AVAILABLE:\n",
    "            try:\n",
    "                run = mlflow.active_run()\n",
    "                if run:\n",
    "                    rid = run.info.run_id[:8]\n",
    "            except Exception:\n",
    "                pass\n",
    "        return rid or datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    def _run_subdir(self) -> Path:\n",
    "\n",
    "        model = self._active_model_name or \"MODEL\"\n",
    "        return Path(self.processed_root) / f\"{model}-{self._active_run_stub()}\"\n",
    "\n",
    "    def _log_df_and_plot(self, df, fig, base_name: str):\n",
    "\n",
    "        root = self._run_subdir()\n",
    "        data_dir = self._safe_mkdir(root / \"data\")\n",
    "        plot_dir = self._safe_mkdir(root / \"plots\")\n",
    "\n",
    "        csv_path = data_dir / f\"{base_name}.csv\"\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        if _MLFLOW_AVAILABLE:\n",
    "            try:\n",
    "                mlflow.log_artifact(str(csv_path), artifact_path=\"data\")\n",
    "            except Exception as e:\n",
    "                print(f\"MLflow log_artifact(data) failed: {e}\")\n",
    "\n",
    "        png_path = plot_dir / f\"{base_name}.png\"\n",
    "        fig.savefig(png_path, bbox_inches=\"tight\", dpi=150)\n",
    "        if _MLFLOW_AVAILABLE:\n",
    "            try:\n",
    "                mlflow.log_artifact(str(png_path), artifact_path=\"plots\")\n",
    "            except Exception as e:\n",
    "                print(f\"MLflow log_artifact(plots) failed: {e}\")\n",
    "    \n",
    "    def _log_metrics(self, metrics: dict, base_name: str = \"eval_metrics\"):\n",
    "        \"\"\"\n",
    "        Save metrics locally and to MLflow.\n",
    "        Local:  processed-data/<MODEL>-<runid>/metrics/<base_name>.{csv,json}\n",
    "        MLflow: log_metric(...) and log_artifact(...) under artifacts/metrics/\n",
    "        \"\"\"\n",
    "        # local save\n",
    "        root = self._run_subdir()\n",
    "        met_dir = self._safe_mkdir(root / \"metrics\")\n",
    "        json_path = met_dir / f\"{base_name}.json\"\n",
    "        csv_path  = met_dir / f\"{base_name}.csv\"\n",
    "\n",
    "        with open(json_path, \"w\") as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "        pd.DataFrame([metrics]).to_csv(csv_path, index=False)\n",
    "\n",
    "        # mlflow\n",
    "        if _MLFLOW_AVAILABLE:\n",
    "            try:\n",
    "                # log individual metrics\n",
    "                for k, v in metrics.items():\n",
    "                    try:\n",
    "                        mlflow.log_metric(k, float(v), step=step)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                # also keep a copy of the files under artifacts/metrics\n",
    "                mlflow.log_artifact(str(json_path), artifact_path=\"metrics\")\n",
    "                mlflow.log_artifact(str(csv_path),  artifact_path=\"metrics\")\n",
    "            except Exception as e:\n",
    "                print(f\"MLflow metric logging failed: {e}\")\n",
    "\n",
    "    def mirror_artifacts_to_datalake(self, dest_base_uri: str = \"dbfs:/mnt/processed-data/mlflow\"):\n",
    "\n",
    "        if not _MLFLOW_AVAILABLE:\n",
    "            print(\"MLflow not available; skipping mirror.\")\n",
    "            return\n",
    "\n",
    "        run = mlflow.active_run()\n",
    "        if run is None:\n",
    "            print(\"No active MLflow run; skipping mirror.\")\n",
    "            return\n",
    "        \n",
    "        src_root = mlflow.get_artifact_uri()\n",
    "        short_id = run.info.run_id[:8]\n",
    "        model = (self._active_model_name or \"MODEL\").upper()\n",
    "        dest_uri = f\"{dest_base_uri.rstrip('/')}/{model}-{short_id}\"\n",
    "\n",
    "        try:\n",
    "            dbutils.fs.mkdirs(dest_uri)\n",
    "            dbutils.fs.cp(src_root, dest_uri, recurse=True)\n",
    "            print(f\"Mirrored MLflow artifacts → {dest_uri}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not mirror MLflow artifacts: {e}\")\n",
    "\n",
    "\n",
    "    #################\n",
    "    # COMMON METHODS\n",
    "    #################\n",
    "    def fix_nan_dataset(self, file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        df_fixed = df.copy()\n",
    "\n",
    "        excluded_indices = df[\n",
    "            df[\"Demand_D+1\"].isnull() | df[\"Demand_D+6\"].isnull()\n",
    "        ].index\n",
    "\n",
    "        demand_cols = [\"Demand_D+1\", \"Demand_D+6\"]\n",
    "        d_cols = [\n",
    "            col\n",
    "            for col in df.columns\n",
    "            if any(token in col for token in [\"D+1\", \"D\", \"D-1\"]) and col not in demand_cols\n",
    "        ]\n",
    "        non_d_cols = [col for col in df.columns if col not in d_cols + demand_cols]\n",
    "\n",
    "        for col in d_cols:\n",
    "            missing_idx = df.index[df[col].isnull() & ~df.index.isin(excluded_indices)]\n",
    "\n",
    "            for idx in missing_idx:\n",
    "\n",
    "                for alt_col in d_cols:\n",
    "                    if alt_col != col and pd.notnull(df.at[idx, alt_col]):\n",
    "                        df_fixed.at[idx, col] = df.at[idx, alt_col]\n",
    "                        break\n",
    "\n",
    "        for col in non_d_cols:\n",
    "            missing_idx = df.index[df[col].isnull() & ~df.index.isin(excluded_indices)]\n",
    "\n",
    "            for idx in missing_idx:\n",
    "                prev_idx = idx - 1\n",
    "                next_idx = idx + 1\n",
    "\n",
    "                while prev_idx >= 0 and pd.isnull(df.at[prev_idx, col]):\n",
    "                    prev_idx -= 1\n",
    "\n",
    "                while next_idx < len(df) and pd.isnull(df.at[next_idx, col]):\n",
    "                    next_idx += 1\n",
    "\n",
    "                prev_val = df.at[prev_idx, col] if prev_idx >= 0 else np.nan\n",
    "                next_val = df.at[next_idx, col] if next_idx < len(df) else np.nan\n",
    "\n",
    "                if pd.notnull(prev_val) and pd.notnull(next_val):\n",
    "                    df_fixed.at[idx, col] = (prev_val + next_val) / 2\n",
    "                elif pd.notnull(prev_val):\n",
    "                    df_fixed.at[idx, col] = prev_val\n",
    "                elif pd.notnull(next_val):\n",
    "                    df_fixed.at[idx, col] = next_val\n",
    "\n",
    "        return df_fixed\n",
    "\n",
    "    def load_and_split_data(\n",
    "        self,\n",
    "        file_path,\n",
    "        target=\"Demand_D+1\",\n",
    "        split_ratio=0.8,\n",
    "        fix_nan=False,\n",
    "        target_shift=0,\n",
    "        apply_supervised=False,):\n",
    "\n",
    "        if fix_nan:\n",
    "            df = self.fix_nan_dataset(file_path)\n",
    "        else:\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "        df['Applicable For'] = pd.to_datetime(df['Applicable For'])\n",
    "        df.set_index(\"Applicable For\", inplace=True)\n",
    "\n",
    "        if target not in df.columns:\n",
    "            raise ValueError(f\"Target column '{target}' not found.\")\n",
    "\n",
    "        if apply_supervised:\n",
    "            df, _, _ = self.prepare_lstm_supervised(df, target=target, target_shift=target_shift)\n",
    "\n",
    "        total_len = len(df)\n",
    "        train_len = int(total_len * split_ratio)\n",
    "\n",
    "        df_train_raw = df.iloc[:train_len]\n",
    "        df_test_raw = df.iloc[train_len:]\n",
    "\n",
    "        train_df = df_train_raw.dropna()\n",
    "        test_df = df_test_raw.dropna()\n",
    "\n",
    "        future_df = df_test_raw[df_test_raw[target].isna()]\n",
    "\n",
    "        self.df = df\n",
    "        self.train_df_3df = train_df\n",
    "        self.test_df_3df = test_df\n",
    "        self.future_df_3df = future_df\n",
    "        self.target_col = target\n",
    "\n",
    "        print(\"Data Summary (target = {})\".format(target))\n",
    "        print(f\"Train Range : {train_df.index.min()} to {train_df.index.max()} ({len(train_df)} rows)\")\n",
    "        print(f\"Test Range  : {test_df.index.min()} to {test_df.index.max()} ({len(test_df)} rows)\")\n",
    "\n",
    "        if len(future_df) > 0:\n",
    "            print(f\"Future Forecast Range : {future_df.index.min()} to {future_df.index.max()} ({len(future_df)} rows)\")\n",
    "        else:\n",
    "            print(\"Future Forecast Range : 0 Rows\")\n",
    "\n",
    "        return train_df, test_df, future_df\n",
    "\n",
    "    def find_best_order(self, seasonal=True, m=7):\n",
    "\n",
    "        if self.df_prepared is None or self.target_col is None:\n",
    "            raise ValueError(\"Call load_and_split_data() before find_best_order().\")\n",
    "\n",
    "        print(\"Running auto_arima...\")\n",
    "        model = auto_arima(\n",
    "            self.df_prepared[self.target_col],\n",
    "            exogenous=self.df_prepared[self.exog_cols] if self.exog_cols else None,\n",
    "            seasonal=seasonal,\n",
    "            m=m,\n",
    "            trace=True,\n",
    "            error_action='ignore',\n",
    "            suppress_warnings=True,\n",
    "        )\n",
    "        print(\"Best order:\", model.order)\n",
    "        print(\"Best seasonal order:\", model.seasonal_order)\n",
    "        return model.order, model.seasonal_order\n",
    "\n",
    "    def normalize_data(self, df, feature_cols, target_col):\n",
    "        scaler_X = MinMaxScaler()\n",
    "        scaler_y = MinMaxScaler()\n",
    "\n",
    "        X_scaled = scaler_X.fit_transform(df[feature_cols])\n",
    "        y_scaled = scaler_y.fit_transform(df[[target_col]])\n",
    "\n",
    "        return X_scaled, y_scaled, scaler_X, scaler_y\n",
    "\n",
    "    def denormalize_data(self, preds_scaled, scaler_y):\n",
    "        return scaler_y.inverse_transform(preds_scaled)\n",
    "\n",
    "    def create_sequences(self, X, y, sequence_length=30):\n",
    "        X_seq, y_seq = [], []\n",
    "        for i in range(sequence_length, len(X)):\n",
    "            X_seq.append(X[i - sequence_length : i])\n",
    "            y_seq.append(y[i])\n",
    "        return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "    def prepare_lstm_supervised(self, df, target, target_shift=0):\n",
    "        df = df.copy()\n",
    "        additional_features = []\n",
    "\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df.index):\n",
    "            if 'Applicable For' in df.columns:\n",
    "                df['Applicable For'] = pd.to_datetime(df['Applicable For'])\n",
    "                df.set_index('Applicable For', inplace=True)\n",
    "            else:\n",
    "                raise ValueError(\"Datetime index or 'Applicable For' column required.\")\n",
    "\n",
    "        if target_shift > 0:\n",
    "            df['Target_Shift_Ac'] = df[target].shift(target_shift)\n",
    "            additional_features.append('Target_Shift_Ac')\n",
    "            df['day_of_week'] = df.index.dayofweek\n",
    "            df['month_of_year'] = df.index.month\n",
    "            additional_features.extend(['day_of_week', 'month_of_year'])\n",
    "\n",
    "        # Return\n",
    "        y = df[target]\n",
    "        return df, y, additional_features\n",
    "\n",
    "    def evaluate_model(self, y_true, y_pred, title=\"Model Evaluation\", log_to_mlflow=False):\n",
    "\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "        print(f\"MAE: {mae:.2f}\")\n",
    "        print(f\"RMSE: {rmse:.2f}\")\n",
    "        print(f\"MAPE: {mape:.2f}\")\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(y_true, label=\"Actual\")\n",
    "        plt.plot(y_pred, label=\"Predicted\")\n",
    "        plt.title(title)\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Value\")\n",
    "        plt.legend()\n",
    "\n",
    "        #MLflow logging\n",
    "        if log_to_mlflow and _MLFLOW_AVAILABLE:\n",
    "            mlflow.log_metrics({\n",
    "                \"mae\": float(mae),\n",
    "                \"rmse\": float(rmse),\n",
    "                \"mape\": float(mape),\n",
    "            })\n",
    "            safe_name = title.replace(\" \", \"_\").lower() + \".png\"\n",
    "            mlflow.log_figure(plt.gcf(), f\"plots/{safe_name}\")\n",
    "    \n",
    "        try:\n",
    "            idx = getattr(y_true, \"index\", pd.RangeIndex(len(y_true)))\n",
    "            plot_df = pd.DataFrame({\n",
    "                \"timestamp\": pd.Index(idx),\n",
    "                \"actual\": np.asarray(y_true).reshape(-1),\n",
    "                \"pred\":   np.asarray(y_pred).reshape(-1),\n",
    "            })\n",
    "\n",
    "            self._log_df_and_plot(df=plot_df, fig=plt.gcf(), base_name=\"forecast_vs_actual\")\n",
    "        except Exception as e:\n",
    "            print(f\"Saving plot data failed (forecast_vs_actual): {e}\")\n",
    "        \n",
    "        metrics = {\"mae\": float(mae), \"rmse\": float(rmse), \"mape\": float(mape)}\n",
    "        self._log_metrics(metrics, base_name=\"evaluation\")\n",
    "\n",
    "        plt.show()\n",
    "        return {\"mae\": float(mae), \"rmse\": float(rmse), \"mape\": float(mape)}\n",
    "\n",
    "    def forecast_future(self,model=None,steps=None,exog_future=None,future_forecast_series=None,log_to_mlflow: bool = False,):\n",
    "\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"No trained model found.\")\n",
    "\n",
    "        if hasattr(self.model, \"forecast\"):\n",
    "            if exog_future is None:\n",
    "                exog_future = self.exog_future\n",
    "\n",
    "            if exog_future is None or exog_future.empty:\n",
    "                raise ValueError(\"No future exogenous data provided.\")\n",
    "\n",
    "            steps = steps or len(exog_future)\n",
    "            # Forecast calculation\n",
    "            forecast = self.model.forecast(steps=steps, exog=exog_future)\n",
    "            print(exog_future.index)\n",
    "            forecast.index = exog_future.index[:steps]\n",
    "            print(f\"Forecast Values :- \")\n",
    "            print(forecast)\n",
    "\n",
    "        else:\n",
    "            # for LSTM\n",
    "            if not hasattr(self, \"future_forecast_series\") or self.future_forecast_series is None:\n",
    "                raise ValueError(\"No LSTM future forecast found. Train LSTM with forecast_after_train=True.\")\n",
    "            forecast = self.future_forecast_series\n",
    "            steps = len(forecast)\n",
    "\n",
    "        forecast_start_date = forecast.index.min()\n",
    "        recent_actuals = self.df[self.target_col].dropna()\n",
    "        recent_actuals = recent_actuals[recent_actuals.index < forecast_start_date].iloc[-steps:]\n",
    "        print(f\"Recent Values :- \")\n",
    "        print(recent_actuals)\n",
    "\n",
    "        # Ensure recent_actuals is a Series\n",
    "        recent_actuals = recent_actuals.rename(\"value\")\n",
    "        recent_actuals = recent_actuals.to_frame()\n",
    "        recent_actuals[\"type\"] = \"Actual\"\n",
    "        # Ensure forecast is a Series\n",
    "        forecast = forecast.rename(\"value\")\n",
    "        forecast = forecast.to_frame()\n",
    "        forecast[\"type\"] = \"Forecast\"\n",
    "\n",
    "        # Combine into one DataFrame\n",
    "        plot_df = pd.concat([recent_actuals, forecast])\n",
    "        # Reindex to continuous daily dates\n",
    "        full_index = pd.date_range(start=plot_df.index.min(), end=plot_df.index.max(), freq='D')\n",
    "        plot_df = plot_df.reindex(full_index)\n",
    "        # Fill missing 'type' if needed\n",
    "        plot_df[\"type\"] = plot_df[\"type\"].fillna(method=\"ffill\")\n",
    "                \n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        # Plot each type with separate color/linestyle\n",
    "        for label, df in plot_df.groupby(\"type\"):\n",
    "            plt.plot(\n",
    "                df.index,\n",
    "                df[\"value\"],\n",
    "                label=label,\n",
    "                linestyle=\"--\" if label == \"Forecast\" else \"-\",\n",
    "                color=\"orange\" if label == \"Forecast\" else \"royalblue\",\n",
    "            )\n",
    "        plt.plot(\n",
    "            [recent_actuals.index[-1], forecast.index[0]],\n",
    "            [recent_actuals[\"value\"].iloc[-1], forecast[\"value\"].iloc[0]],\n",
    "            linestyle='--',\n",
    "            color='orange',\n",
    "        )\n",
    "\n",
    "        import matplotlib.dates as mdates\n",
    "        date_fmt = mdates.DateFormatter('%d-%m-%Y')\n",
    "        plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "        plt.title(f\"Next {len(forecast)}-Day Forecast for {self.target_col}\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(self.target_col)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.legend()\n",
    "\n",
    "        #Save artifacts\n",
    "        try:\n",
    "            next_df = forecast.reset_index().rename(columns={\n",
    "                forecast.index.name or \"index\": \"timestamp\",\n",
    "                \"value\": \"forecast\",\n",
    "            })\n",
    "            self._log_df_and_plot(df=next_df, fig=plt.gcf(), base_name=\"next_n_days\")\n",
    "\n",
    "            recent_and_forecast = plot_df.reset_index().rename(columns={\"index\": \"timestamp\"})\n",
    "            \n",
    "            self._log_df_and_plot(df=recent_and_forecast, fig=plt.gcf(), base_name=\"recent_and_forecast\")\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] Saving plot data failed: {e}\")\n",
    "\n",
    "        #MLflow logging#\n",
    "        if log_to_mlflow and _MLFLOW_AVAILABLE:\n",
    "            mlflow.log_figure(plt.gcf(), \"plots/forecast.png\")\n",
    "\n",
    "        return forecast\n",
    "\n",
    "    ##################################\n",
    "    # SARIMAX TRAINING / FORECASTING\n",
    "    ##################################\n",
    "    def train_sarimax(\n",
    "        self,\n",
    "        order=None,\n",
    "        seasonal_order=None,\n",
    "        exog_vars=None,\n",
    "        auto=False,\n",
    "        seasonal=True,\n",
    "        m=7,\n",
    "        ac_fo_mode=False,\n",
    "        split_ratio=0.8,\n",
    "        file_path=None,\n",
    "        target=\"Demand_D+1\",\n",
    "        fix_nan=False,\n",
    "        forecaster_after_train=True,\n",
    "        use_mlflow=False,\n",
    "        flow_num=None,\n",
    "        tag=None):\n",
    "        \"\"\"\n",
    "        Train a SARIMAX model using one of 4 scenarios.\n",
    "        If use_mlflow=True, logs params/metrics/plots/model to MLflow.\n",
    "        \"\"\"\n",
    "        run_name = f\"SARIMAX_{target}_{tag}_Run_{flow_num}\"\n",
    "        with self._mlflow_run(use_mlflow, run_name):\n",
    "            self._active_model_name = \"SARIMAX\"\n",
    "            \n",
    "            train_df, test_df, future_df = self.load_and_split_data(\n",
    "                file_path=file_path,\n",
    "                target=target,\n",
    "                split_ratio=split_ratio,\n",
    "                fix_nan=fix_nan,\n",
    "            )\n",
    "\n",
    "            #mflow logging parameters\n",
    "            if use_mlflow and _MLFLOW_AVAILABLE:\n",
    "                mlflow.set_tags({\n",
    "                    \"framework\": \"statsmodels\",\n",
    "                    \"mode\": \"AC/FO\" if ac_fo_mode else \"SameExog\",\n",
    "                })\n",
    "                mlflow.log_dict(\n",
    "                    {\n",
    "                        \"train_rows\": int(len(train_df)),\n",
    "                        \"test_rows\": int(len(test_df)),\n",
    "                        \"future_rows\": int(len(future_df)),\n",
    "                        \"train_start\": str(train_df.index.min()),\n",
    "                        \"train_end\": str(train_df.index.max()),\n",
    "                        \"test_start\": str(test_df.index.min()),\n",
    "                        \"test_end\": str(test_df.index.max()),\n",
    "                    },\n",
    "                    \"data/summary.json\",\n",
    "                )\n",
    "                mlflow.log_params(\n",
    "                    {\n",
    "                        \"model_type\": \"SARIMAX\",\n",
    "                        \"target\": target,\n",
    "                        \"split_ratio\": split_ratio,\n",
    "                        \"fix_nan\": fix_nan,\n",
    "                        \"auto_order\": auto,\n",
    "                        \"seasonal\": seasonal,\n",
    "                        \"m\": m,\n",
    "                        \"ac_fo_mode\": ac_fo_mode,\n",
    "                        \"exog_vars_ac\": \",\".join(exog_vars['ac'])\n",
    "                        if ac_fo_mode and exog_vars and 'ac' in exog_vars\n",
    "                        else \"\",\n",
    "                        \"exog_vars_fo\": \",\".join(exog_vars['fo'])\n",
    "                        if ac_fo_mode and exog_vars and 'fo' in exog_vars\n",
    "                        else \"\",\n",
    "                        \"exog_vars_same\": \",\".join(exog_vars)\n",
    "                        if (not ac_fo_mode and exog_vars)\n",
    "                        else \"\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            if ac_fo_mode:\n",
    "                # separate AC (train) and FO (test/future)\n",
    "                exog_train = (\n",
    "                    train_df[exog_vars['ac']] if exog_vars and 'ac' in exog_vars else None\n",
    "                )\n",
    "                exog_test = (\n",
    "                    test_df[exog_vars['fo']] if exog_vars and 'fo' in exog_vars else None\n",
    "                )\n",
    "                exog_future = (\n",
    "                    future_df[exog_vars['fo']]\n",
    "                    if exog_vars and 'fo' in exog_vars and not future_df.empty\n",
    "                    else pd.DataFrame()\n",
    "                )\n",
    "                self.exog_cols = exog_vars['ac'] if exog_vars and 'ac' in exog_vars else None\n",
    "                print(f\"Exog Train (AC): {exog_train}\")\n",
    "                print(f\"Exog Test (FO): {exog_test}\")\n",
    "                print(f\"Exog Future (FO): {exog_future}\")\n",
    "\n",
    "            else:\n",
    "                # same exog variables for train/test/future\n",
    "                exog_train = train_df[exog_vars] if exog_vars else None\n",
    "                exog_test = test_df[exog_vars] if exog_vars else None\n",
    "                exog_future = (\n",
    "                    future_df[exog_vars] if exog_vars and not future_df.empty else pd.DataFrame()\n",
    "                )\n",
    "                self.exog_cols = exog_vars\n",
    "                print(f\"Exog Train: {exog_train}\")\n",
    "                print(f\"Exog Test: {exog_test}\")\n",
    "                print(f\"Exog Future: {exog_future}\")\n",
    "\n",
    "            #Cleaning\n",
    "            y_train = train_df[target]\n",
    "            y_test = test_df[target]\n",
    "            if exog_train is not None:\n",
    "                valid_idx = exog_train.dropna().index.intersection(y_train.dropna().index)\n",
    "                exog_train = exog_train.loc[valid_idx]\n",
    "                y_train = y_train.loc[valid_idx]\n",
    "\n",
    "            #store for internal use\n",
    "            self.target_col = target\n",
    "            self.df_prepared = train_df\n",
    "            self.exog_future = exog_future\n",
    "\n",
    "            #auto arima order\n",
    "            if auto:\n",
    "                order, seasonal_order = self.find_best_order(seasonal=seasonal, m=m)\n",
    "                if use_mlflow and _MLFLOW_AVAILABLE:\n",
    "                    mlflow.log_params({\n",
    "                        \"order\": str(order),\n",
    "                        \"seasonal_order\": str(seasonal_order),\n",
    "                    })\n",
    "\n",
    "            #fit and training model\n",
    "            self.model = (\n",
    "                SARIMAX(\n",
    "                    endog=y_train,\n",
    "                    exog=exog_train,\n",
    "                    order=order,\n",
    "                    seasonal_order=seasonal_order,\n",
    "                    enforce_stationarity=False,\n",
    "                    enforce_invertibility=False,\n",
    "                    measurement_error=False,\n",
    "                ).fit(disp=False)\n",
    "            )\n",
    "\n",
    "            print(\"SARIMAX model trained.\")\n",
    "\n",
    "            # STEP 6: Predict and evaluate using test set\n",
    "            preds = self.model.predict(\n",
    "                start=len(y_train),\n",
    "                end=len(y_train) + len(y_test) - 1,\n",
    "                exog=exog_test,\n",
    "            )\n",
    "            # Fix index misalignment\n",
    "            preds.index = y_test.index\n",
    "            self.evaluate_model(\n",
    "                y_test,\n",
    "                preds,\n",
    "                title=\"SARIMAX Test Evaluation\",\n",
    "                log_to_mlflow=use_mlflow and _MLFLOW_AVAILABLE,\n",
    "            )\n",
    "\n",
    "            # STEP 7: Log model (optional)\n",
    "            if use_mlflow and _MLFLOW_AVAILABLE:\n",
    "                try:\n",
    "                    if _HAS_SM_FLAVOR:\n",
    "                        mlflow.statsmodels.log_model(self.model, name=\"model\")\n",
    "                    else:\n",
    "                        with open(\"sarimax.pkl\", \"wb\") as f:\n",
    "                            pickle.dump(self.model, f)\n",
    "                        mlflow.log_artifact(\"sarimax.pkl\", artifact_path=\"model\")\n",
    "                except Exception as e:\n",
    "                    print(f\"MLflow model logging failed: {e}\")\n",
    "\n",
    "            # STEP 8: Future forecast using general plotting method\n",
    "            if forecaster_after_train and not self.exog_future.empty:\n",
    "                self.forecast_future(\n",
    "                    model=self.model,\n",
    "                    steps=len(self.exog_future),\n",
    "                    exog_future=self.exog_future,\n",
    "                    log_to_mlflow=use_mlflow and _MLFLOW_AVAILABLE,\n",
    "                )\n",
    "\n",
    "            if use_mlflow and _MLFLOW_AVAILABLE:\n",
    "               self.mirror_artifacts_to_datalake(\"dbfs:/mnt/processed-data/mlflow\")\n",
    "            \n",
    "        return self.model, preds, self.exog_future\n",
    "    \n",
    "    \n",
    "    ##################################\n",
    "    # Deep Learning Models TRAINING AND FORECASTING\n",
    "    ##################################\n",
    "    def train_dlm(\n",
    "        self,\n",
    "        file_path,\n",
    "        target,\n",
    "        DLM=\"LSTM\",\n",
    "        target_shift=0,\n",
    "        feature_cols=None,\n",
    "        split_ratio=0.8,\n",
    "        sequence_length=30,\n",
    "        units=64,\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        dense=1,\n",
    "        random_seed=42,\n",
    "        forecast_after_train=True,\n",
    "        fix_nan=False,\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"mse\",\n",
    "        use_mlflow=False,\n",
    "        flow_num=None,\n",
    "        tag=None,):\n",
    "        \"\"\"\n",
    "        Unified Deep Learning Model (DLM) trainer for sequence models.\n",
    "        Chooses between LSTM / SimpleRNN / GRU based on `DLM`.\n",
    "        Mirrors the behavior of train_lstm/train_rnn/train_gru.\n",
    "        \"\"\"\n",
    "        # ---- choose recurrent layer ----\n",
    "        from tensorflow.keras.layers import LSTM as _LSTM, SimpleRNN as _RNN, GRU as _GRU\n",
    "        \n",
    "        if isinstance(DLM, str):\n",
    "            key = DLM.strip().upper()\n",
    "            layer_map = {\"LSTM\": _LSTM, \"RNN\": _RNN, \"GRU\": _GRU}\n",
    "            if key not in layer_map:\n",
    "                raise ValueError(f\"DLM must be one of {{'LSTM','RNN','GRU'}} or a Keras layer class. Got: {DLM}\")\n",
    "            layer_cls = layer_map[key]\n",
    "        else:\n",
    "            # Allow passing an actual Keras layer class (e.g., GRU)\n",
    "            layer_cls = DLM\n",
    "            key = getattr(layer_cls, \"__name__\", \"DLM\")\n",
    "\n",
    "        # ---- seeding ----\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        tf.random.set_seed(random_seed)\n",
    "\n",
    "        # ---- MLflow run name ----\n",
    "        run_name = f\"{key}_{target}_{tag}_Run_{flow_num}\"\n",
    "        with self._mlflow_run(use_mlflow, run_name):\n",
    "            self._active_model_name = key\n",
    "            # ---- load & split ----\n",
    "            train_df, test_df, future_df = self.load_and_split_data(\n",
    "                file_path=file_path,\n",
    "                target=target,\n",
    "                split_ratio=split_ratio,\n",
    "                fix_nan=fix_nan,\n",
    "                target_shift=target_shift,\n",
    "                apply_supervised=True,\n",
    "            )\n",
    "            df = pd.concat([train_df, test_df])\n",
    "\n",
    "            if feature_cols is None:\n",
    "                raise ValueError(\"You must provide feature_cols list.\")\n",
    "\n",
    "            # ---- MLflow metadata ----\n",
    "            if use_mlflow and _MLFLOW_AVAILABLE:\n",
    "                mlflow.set_tags({\"framework\": \"keras\"})\n",
    "                mlflow.log_dict(\n",
    "                    {\n",
    "                        \"train_rows\": int(len(train_df)),\n",
    "                        \"test_rows\": int(len(test_df)),\n",
    "                        \"future_rows\": int(len(future_df)),\n",
    "                        \"train_start\": str(train_df.index.min()),\n",
    "                        \"train_end\": str(train_df.index.max()),\n",
    "                        \"test_start\": str(test_df.index.min()),\n",
    "                        \"test_end\": str(test_df.index.max()),\n",
    "                    },\n",
    "                    \"data/summary.json\",\n",
    "                )\n",
    "                mlflow.log_params(\n",
    "                    {\n",
    "                        \"model_type\": key,\n",
    "                        \"target\": target,\n",
    "                        \"split_ratio\": split_ratio,\n",
    "                        \"sequence_length\": sequence_length,\n",
    "                        \"units\": units,\n",
    "                        \"epochs\": epochs,\n",
    "                        \"batch_size\": batch_size,\n",
    "                        \"dense\": dense,\n",
    "                        \"optimizer\": optimizer,\n",
    "                        \"loss\": loss,\n",
    "                        \"target_shift\": target_shift,\n",
    "                        \"fix_nan\": fix_nan,\n",
    "                        \"random_seed\": random_seed,\n",
    "                        \"feature_cols\": \",\".join(feature_cols) if feature_cols else \"\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # ---- engineered features (when shifting) ----\n",
    "            engineered_if_shift = (\n",
    "                [\"Target_Shift_Ac\", \"day_of_week\", \"month_of_year\"] if target_shift > 0 else []\n",
    "            )\n",
    "            feature_cols = feature_cols + [\n",
    "                f for f in engineered_if_shift if f not in feature_cols and f in df.columns\n",
    "            ]\n",
    "\n",
    "            print(f\"Final features used in {key}:\", feature_cols)\n",
    "            print(\"DataFrame columns before normalization\", df.columns.tolist())\n",
    "\n",
    "            df = df.dropna(subset=feature_cols + [target])\n",
    "\n",
    "            # ---- scale + sequences ----\n",
    "            X_scaled, y_scaled, scaler_X, scaler_y = self.normalize_data(df, feature_cols, target)\n",
    "            X_seq, y_seq = self.create_sequences(X_scaled, y_scaled, sequence_length)\n",
    "\n",
    "            split = int(len(X_seq) * split_ratio)\n",
    "            X_train, X_test = X_seq[:split], X_seq[split:]\n",
    "            y_train, y_test = y_seq[:split], y_seq[split:]\n",
    "\n",
    "            # ---- build model ----\n",
    "            model = Sequential()\n",
    "            model.add(layer_cls(units, input_shape=(sequence_length, len(feature_cols))))\n",
    "            model.add(Dense(dense))\n",
    "            model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "            model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "            # ---- eval ----\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred_inv = self.denormalize_data(y_pred, scaler_y)\n",
    "            y_test_inv = self.denormalize_data(y_test, scaler_y)\n",
    "\n",
    "            test_index = df.index[-len(y_test_inv):]\n",
    "            y_test_inv_series = pd.Series(y_test_inv.flatten(), index=test_index)\n",
    "            y_pred_inv_series = pd.Series(y_pred_inv.flatten(), index=test_index)\n",
    "\n",
    "            self.evaluate_model(\n",
    "                y_test_inv_series,\n",
    "                y_pred_inv_series,\n",
    "                title=f\"{key} Forecast vs Actual\",\n",
    "                log_to_mlflow=use_mlflow and _MLFLOW_AVAILABLE,\n",
    "            )\n",
    "\n",
    "            # ---- save state for future forecasting ----\n",
    "            self.model = model\n",
    "            self.scaler_X = scaler_X\n",
    "            self.scaler_y = scaler_y\n",
    "            self.sequence_length = sequence_length\n",
    "            self.feature_cols = feature_cols\n",
    "            self.target_col = target\n",
    "\n",
    "            # ---- future forecast ----\n",
    "            if forecast_after_train and not self.future_df_3df.empty:\n",
    "                print(\"\\nFuture data available. Predicting future values...\")\n",
    "\n",
    "            context_df = pd.concat(\n",
    "                [df.iloc[-self.sequence_length :], self.future_df_3df]\n",
    "            )\n",
    "\n",
    "            X_future_scaled = self.scaler_X.transform(context_df[self.feature_cols])\n",
    "            X_future_seq, _ = self.create_sequences(\n",
    "                X_future_scaled, np.zeros(len(X_future_scaled)), self.sequence_length\n",
    "            )\n",
    "            y_future_scaled = self.model.predict(X_future_seq)\n",
    "            y_future = self.denormalize_data(y_future_scaled, self.scaler_y)\n",
    "\n",
    "            forecast_index = self.future_df_3df.index[: len(y_future)]\n",
    "            future_forecast = pd.Series(y_future.flatten(), index=forecast_index)\n",
    "\n",
    "            print(f\"{key} Future Forecast:\")\n",
    "            print(future_forecast)\n",
    "\n",
    "            self.future_forecast_series = future_forecast\n",
    "\n",
    "            if forecast_after_train:\n",
    "                self.forecast_future(\n",
    "                    model=self.model,\n",
    "                    future_forecast_series=future_forecast,\n",
    "                    log_to_mlflow=use_mlflow and _MLFLOW_AVAILABLE,\n",
    "                )\n",
    "\n",
    "            # ---- log model artifact ----\n",
    "            if use_mlflow and _MLFLOW_AVAILABLE:\n",
    "                try:\n",
    "                    # NOTE: use artifact_path (correct) rather than name=\n",
    "                    mlflow.keras.log_model(self.model, artifact_path=\"model\")\n",
    "                except Exception as e:\n",
    "                    print(f\"MLflow Keras model logging failed: {e}\")\n",
    "\n",
    "                        \n",
    "            if use_mlflow and _MLFLOW_AVAILABLE:\n",
    "                # send this run’s artifacts to the lake\n",
    "                self.mirror_artifacts_to_datalake(\"dbfs:/mnt/processed-data/mlflow\")\n",
    "\n",
    "            return self, model, scaler_X, scaler_y, future_forecast\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9657a4b-0518-4413-96fa-5dbb537471c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "CALL RNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84ea7a22-aab5-4e18-a3ba-28a10703c0ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rnn_forecaster = GasModel()\n",
    "\n",
    "\n",
    "rnn_forecaster.train_dlm(\n",
    "    file_path=master_data_path_dbfs,\n",
    "    target=\"Demand_D+1\",\n",
    "    target_shift=1,\n",
    "    feature_cols=[\"Temp_Fo_D-1(C)\", \"CWV_D-1\"],\n",
    "    #feature_cols=[\"Temp_Fo_D-1(C)\", \"CWV_D-1\", \"SAP_Actual_Day\",\n",
    "    #\"SAP_7D_RAVG\", \"SAP_30D_RAVG\", \"Calorific_Value\"],\n",
    "    split_ratio=0.8,\n",
    "    sequence_length=30,\n",
    "    units=64,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    dense=1,\n",
    "    random_seed=42,\n",
    "    forecast_after_train=True,\n",
    "    fix_nan=True,\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    use_mlflow=True,\n",
    "    flow_num=1,\n",
    "    tag=\"RNN_Bi_Test\",\n",
    "    DLM=\"RNN\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ff7dca8-81be-4c8f-b152-e956a7917559",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "CALL LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4508bc87-f06d-49c7-934a-d13dd7e82116",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lstm_forecaster = GasModel()\n",
    "\n",
    "\n",
    "lstm_forecaster.train_dlm(\n",
    "    file_path=master_data_path_dbfs,\n",
    "    target=\"Demand_D+1\",\n",
    "    target_shift=1,\n",
    "    feature_cols=[\"Temp_Fo_D-1(C)\", \"CWV_D-1\",\"Calorific_Value\"],\n",
    "    #feature_cols=[\"Temp_Fo_D-1(C)\", \"CWV_D-1\", \"SAP_Actual_Day\",\n",
    "    #\"SAP_7D_RAVG\", \"SAP_30D_RAVG\", \"Calorific_Value\"],\n",
    "    split_ratio=0.8,\n",
    "    sequence_length=30,\n",
    "    units=64,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    dense=1,\n",
    "    random_seed=42,\n",
    "    forecast_after_train=True,\n",
    "    fix_nan=True,\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    use_mlflow=True,\n",
    "    flow_num=1,\n",
    "    tag=\"LSTM_Bi_Test\",\n",
    "    DLM=\"LSTM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e33973e2-ee8e-4992-88f9-d6ded477d4d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "CALL GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0978fe69-c47a-465e-91a9-5e6557319fec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gru_forecaster = GasModel()\n",
    "\n",
    "\n",
    "gru_forecaster.train_dlm(\n",
    "    file_path=master_data_path_dbfs,\n",
    "    target=\"Demand_D+1\",\n",
    "    target_shift=1,\n",
    "    feature_cols=[\"Temp_Fo_D-1(C)\", \"CWV_D-1\",\"Calorific_Value\"],\n",
    "    #feature_cols=[\"Temp_Fo_D-1(C)\", \"CWV_D-1\", \"SAP_Actual_Day\",\n",
    "    #\"SAP_7D_RAVG\", \"SAP_30D_RAVG\", \"Calorific_Value\"],\n",
    "    split_ratio=0.8,\n",
    "    sequence_length=30,\n",
    "    units=64,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    dense=1,\n",
    "    random_seed=42,\n",
    "    forecast_after_train=True,\n",
    "    fix_nan=True,\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    use_mlflow=True,\n",
    "    flow_num=1,\n",
    "    tag=\"GRU_Bi_Test\",\n",
    "    DLM=\"GRU\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d733e049-9bb9-432a-86ee-067c4d485140",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "CALL FOR SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2827c9c5-4fe8-4212-b719-f0766669da2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sarimax_forecaster = GasModel()\n",
    "sarimax_forecaster.train_sarimax(\n",
    "    file_path=master_data_path_dbfs,\n",
    "    target=\"Demand_D+1\",\n",
    "    exog_vars= {\n",
    "        \"ac\": [\"Temp_Ac_D+1(C)\", \"CWV_D+1\"],\n",
    "        \"fo\": [\"Temp_Fo_D-1(C)\", \"CWV_D-1\"]},\n",
    "#    exog_vars=[\"SAP_Actual_Day\", \"Temp_Ac_D+1(C)\", \"Temp_Fo_D-1(C)\",\"Calorific_Value\", \"CWV_D\", \"CWV_D-1\", \"CWV_D+1\", \"SAP_30D_RAVG\"],\n",
    "    auto=False,\n",
    "    #(2,1,1)(1,0,1)[7] \n",
    "    order=(2,1,1),\n",
    "    seasonal_order=(1,0,1,7),\n",
    "    ac_fo_mode=True,\n",
    "    forecaster_after_train=True,\n",
    "    split_ratio=0.8,\n",
    "    fix_nan=True,\n",
    "    use_mlflow=True,\n",
    "    flow_num=1,\n",
    "    tag=\"SARIMAX_Bi_Test\"\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LDZ_EA_FO_Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
