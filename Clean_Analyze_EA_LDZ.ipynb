{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f1b03c8-5f8a-4f01-ba4f-9cc1659af68f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "0. IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9d03a7b-6896-4a7d-8d16-f22d13635bf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71875b9f-f7cc-4ce1-95da-46a382cc38fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. AZURE DATA LAKE STORAGE CONFIG KEY FROM VAULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d51b940e-14fa-46b5-80bd-ff2e16edea8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account_name = \"gasprojectsadl\"\n",
    "raw_data_container = \"raw-data\"\n",
    "processed_data_container = \"processed-data\"\n",
    "\n",
    "storage_key = dbutils.secrets.get(scope=\"gas-project-scope\", key=\"gasprojectsadl-key\")\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\",\n",
    "    storage_key\n",
    ")\n",
    "print(\"Configuration complete. Databricks is now connected to your Azure Data Lake.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "922d55de-5f56-4ef5-abe1-a358ba3fca74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. FILE PATH AND LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "397110cd-cfd7-4865-903b-400705e6ffac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "base_path = f\"abfss://{raw_data_container}@{storage_account_name}.dfs.core.windows.net/EA_LDZ_Raw_Data\"\n",
    "\n",
    "demand_file_path = f\"{base_path}/EA_Demand_Actual.xlsx\"\n",
    "cv_file_path = f\"{base_path}/EA_CV.xlsx\"\n",
    "cwv_file_path = f\"{base_path}/EA_CWV.xlsx\"\n",
    "sap_price_file_path = f\"{base_path}/SAP_Price.xlsx\"\n",
    "temp_file_path = f\"{base_path}/Temperature.xlsx\"\n",
    "\n",
    "print(\"File paths defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcb18888-62f7-4ec4-a036-b71f081d5172",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. PROCESSING DEMAND DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f579a958-0a1c-4710-9cfa-494631bdae81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "pd_demand_path = f\"/dbfs/mnt/{raw_data_container}/EA_LDZ_Raw_Data/EA_Demand_Actual.xlsx\"\n",
    "\n",
    "try:\n",
    "    dbutils.fs.mount(\n",
    "        source=f\"wasbs://{raw_data_container}@{storage_account_name}.blob.core.windows.net/\",\n",
    "        mount_point=f\"/mnt/{raw_data_container}\",\n",
    "        extra_configs={f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\": storage_key},\n",
    "    )\n",
    "except Exception as e:\n",
    "    if \"Directory already mounted\" in str(e):\n",
    "        print(\"Directory is already mounted.\")\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "sheet_map_demand = {'D+1': 'Demand_D+1', 'D+6': 'Demand_D+6'}\n",
    "cleaned_demand_dfs = []\n",
    "for sheet, col_name in sheet_map_demand.items():\n",
    "    df = pd.read_excel(pd_demand_path, sheet_name=sheet)\n",
    "    df['Applicable For'] = pd.to_datetime(df['Applicable For'])\n",
    "    df = df[['Applicable For', 'Value']].rename(columns={'Value': col_name})\n",
    "    df.set_index('Applicable For', inplace=True)\n",
    "    cleaned_demand_dfs.append(df)\n",
    "\n",
    "pd_final_demand = pd.concat(cleaned_demand_dfs, axis=1, join='outer').sort_index()\n",
    "df_demand = spark.createDataFrame(pd_final_demand.reset_index())\n",
    "\n",
    "print(\"Demand data processed successfully.\")\n",
    "display(df_demand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "473ea776-97cf-49d0-a1a7-6918ffdd39de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. PROCESSING CV, CWV , SAP and Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c179c2b-a130-4c45-9c3d-aa21f09f76d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_single_sheet_excel(file_name, new_col_name, sheet_name='Sheet1'):\n",
    "    pd_path = f\"/dbfs/mnt/{raw_data_container}/EA_LDZ_Raw_Data/{file_name}\"\n",
    "    df = pd.read_excel(pd_path, sheet_name=sheet_name)\n",
    "    df['Applicable For'] = pd.to_datetime(df['Applicable For'])\n",
    "    df = df[['Applicable For', 'Value']].rename(columns={'Value': new_col_name})\n",
    "    df.set_index('Applicable For', inplace=True)\n",
    "    return spark.createDataFrame(df.reset_index())\n",
    "\n",
    "def process_multi_sheet_excel(file_name, sheet_map):\n",
    "    pd_path = f\"/dbfs/mnt/{raw_data_container}/EA_LDZ_Raw_Data/{file_name}\"\n",
    "    cleaned_dfs = []\n",
    "    for sheet, col_name in sheet_map.items():\n",
    "        df = pd.read_excel(pd_path, sheet_name=sheet)\n",
    "        df['Applicable For'] = pd.to_datetime(df['Applicable For'])\n",
    "        df = df[['Applicable For', 'Value']].rename(columns={'Value': col_name})\n",
    "        df.set_index('Applicable For', inplace=True)\n",
    "        cleaned_dfs.append(df)\n",
    "    pd_final = pd.concat(cleaned_dfs, axis=1, join='outer').sort_index()\n",
    "    return spark.createDataFrame(pd_final.reset_index())\n",
    "\n",
    "df_cv = process_single_sheet_excel(\"EA_CV.xlsx\", \"Calorific_Value\")\n",
    "print(\"CV data processed.\")\n",
    "\n",
    "sheet_map_cwv = {'EA_CWV_D-1': 'CWV_D-1', 'EA_CWV_D': 'CWV_D', 'EA_CWV_D+1': 'CWV_D+1'}\n",
    "df_cwv = process_multi_sheet_excel(\"EA_CWV.xlsx\", sheet_map_cwv)\n",
    "print(\"CWV data processed.\")\n",
    "\n",
    "sheet_map_sap = {'SAP_Actual_Day': 'SAP_Actual_Day', 'SAP_Highest_18': 'SAP_Highest_18', 'SAP_7D_RAVG': 'SAP_7D_RAVG', 'SAP_30D_RAVG': 'SAP_30D_RAVG'}\n",
    "df_sap = process_multi_sheet_excel(\"SAP_Price.xlsx\", sheet_map_sap)\n",
    "print(\"SAP Price data processed.\")\n",
    "\n",
    "sheet_map_temp = {'Ac_D+1': 'Temp_Ac_D+1(C)', 'Fo_D': 'Temp_Fo_D(C)', 'Fo_D-1': 'Temp_Fo_D-1(C)', 'SN_D': 'Temp_SN_D(C)', 'SN_D+1': 'Temp_SN_D+1(C)', 'SN_D-1': 'Temp_SN_D-1(C)'}\n",
    "df_temp = process_multi_sheet_excel(\"Temperature.xlsx\", sheet_map_temp)\n",
    "print(\"Temperature data processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8d719b7-6fbe-4495-9819-bba38305e1f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. OUTER JOIN ALL TO MASTER DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2107ee87-4ee8-4a86-8b5f-adc7a1400c60",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753556106211}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, date_format\n",
    "\n",
    "master_df_raw = df_demand \\\n",
    "    .join(df_cv, on='Applicable For', how='outer') \\\n",
    "    .join(df_cwv, on='Applicable For', how='outer') \\\n",
    "    .join(df_sap, on='Applicable For', how='outer') \\\n",
    "    .join(df_temp, on='Applicable For', how='outer')\n",
    "\n",
    "master_df_formatted = master_df_raw.withColumn(\"Applicable For\", to_date(col(\"Applicable For\")))\n",
    "\n",
    "master_df_final = master_df_formatted.orderBy('Applicable For')\n",
    "\n",
    "print(\"All data has been merged and formatted.\")\n",
    "print(\"Displaying the final master DataFrame (nulls will appear as blank):\")\n",
    "display(master_df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12bb448e-bdae-4ce8-9f48-295a6eeead81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. SAVE MASTER DF TO PROCESSED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74290661-3152-439f-a214-f9650755dd28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.dbutils import DBUtils\n",
    "\n",
    "temp_output_path = f\"abfss://{processed_data_container}@{storage_account_name}.dfs.core.windows.net/tmp_final_csv\"\n",
    "\n",
    "final_output_path = f\"abfss://{processed_data_container}@{storage_account_name}.dfs.core.windows.net/final_ea_ldz_master_csv/master_ea_ldz.csv\"\n",
    "\n",
    "master_df_final.coalesce(1).write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"nullValue\", \"\") \\\n",
    "    .csv(temp_output_path)\n",
    "\n",
    "dbutils = DBUtils(SparkSession.builder.getOrCreate())\n",
    "fs = dbutils.fs\n",
    "\n",
    "files = fs.ls(temp_output_path)\n",
    "part_file = [f.path for f in files if f.name.startswith(\"part-\") and f.name.endswith(\".csv\")][0]\n",
    "\n",
    "fs.mv(part_file, final_output_path)\n",
    "\n",
    "fs.rm(temp_output_path, recurse=True)\n",
    "\n",
    "print(f\"Successfully saved as: {final_output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ac00d9b-048e-4a38-9cda-ab2b3ccf86b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. CORRELATION MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb2c981-ab2d-4411-bfeb-d451c7dcda54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_corr_pandas = master_df_final.toPandas()\n",
    "df_corr_pandas.set_index('Applicable For', inplace=True)\n",
    "df_numeric = df_corr_pandas.select_dtypes(include=['number']).fillna(method='ffill')\n",
    "\n",
    "corr_matrix = df_numeric.corr()\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "heatmap = sns.heatmap(corr_matrix,annot=True,fmt=\".2f\",cmap='coolwarm',square=True,linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix - EA LDZ Forecasting Data\", fontsize=16)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a89fd4d0-9465-4b47-b06f-017a955e5e60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "8. TRENDS ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b981ebd-8436-43c0-89cd-f13ef4464dfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "df_plot_pandas = master_df_final.toPandas()\n",
    "df_plot_pandas['Applicable For'] = pd.to_datetime(df_plot_pandas['Applicable For'])\n",
    "df_plot_pandas.set_index('Applicable For', inplace=True)\n",
    "plot_df = df_plot_pandas[['Demand_D+1', 'Temp_Ac_D+1(C)', 'Calorific_Value']]\n",
    "monthly_df = plot_df.resample('M').mean()\n",
    "\n",
    "plt.figure(figsize=(16, 7))\n",
    "plt.plot(monthly_df.index, monthly_df['Demand_D+1'], label='Demand_D+1', linewidth=2)\n",
    "plt.plot(monthly_df.index, monthly_df['Temp_Ac_D+1(C)'], label='Temp_Ac_D+1(C)', linewidth=2)\n",
    "plt.plot(monthly_df.index, monthly_df['Calorific_Value'], label='Calorific Value', linewidth=2)\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%Y'))\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "plt.title('Monthly Trends: Demand vs Temp (Ac_D+1) vs CV', fontsize=14)\n",
    "plt.xlabel('Month-Year (MM-YYYY)')\n",
    "plt.ylabel('Values (scaled individually)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Clean_Analyze_EA_LDZ",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
